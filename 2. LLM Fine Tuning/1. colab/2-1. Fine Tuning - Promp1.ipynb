{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3380e0",
   "metadata": {},
   "source": [
    "# Prefix Tuning과 Prompt Tuning 완전 정복\n",
    "\n",
    "## 학습 목표\n",
    "이 강의를 통해 다음을 배우게 됩니다:\n",
    "1. **Prefix Tuning**과 **Prompt Tuning**의 기본 개념\n",
    "2. 두 방법이 왜 필요한지, 언제 사용하는지\n",
    "3. EXAONE-3.5-7.8B-Instruct 모델을 활용한 실제 구현\n",
    "4. 한국어 데이터셋을 이용한 실습\n",
    "\n",
    "---\n",
    "\n",
    "## 목차\n",
    "1. [Parameter-Efficient Fine-Tuning(PEFT) 소개](#1-parameter-efficient-fine-tuningpeft-소개)\n",
    "2. [Prefix Tuning 이해하기](#2-prefix-tuning-이해하기)\n",
    "3. [Prompt Tuning 이해하기](#3-prompt-tuning-이해하기)\n",
    "4. [실습: EXAONE 모델로 PrefixTuningConfig와 PromptTuningConfig 예제](#4-실습-exaone-모델로-prefixtungconfig와-prompttuningconfig-예제)\n",
    "5. [데이터 전처리 및 학습](#5-데이터-전처리-및-학습)\n",
    "6. [모델 테스트 및 성능 평가](#6-모델-테스트-및-성능-평가)\n",
    "7. [성능 비교 및 정리](#7-성능-비교-및-정리)\n",
    "8. [실습 과제 및 다음 단계](#8-실습-과제-및-다음-단계)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d76e57",
   "metadata": {},
   "source": [
    "## 1. Parameter-Efficient Fine-Tuning(PEFT) 소개\n",
    "\n",
    "### 전통적인 Fine-Tuning의 문제점\n",
    "\n",
    "기존의 **전체 모델 Fine-Tuning**은 다음과 같은 문제가 있습니다:\n",
    "\n",
    "- **메모리 부족**: 대형 모델(7B, 13B 파라미터)의 모든 파라미터를 업데이트하려면 엄청난 GPU 메모리가 필요\n",
    "- **저장 공간**: 각 태스크마다 전체 모델을 별도로 저장해야 함\n",
    "- **학습 시간**: 모든 파라미터를 업데이트하므로 학습이 오래 걸림\n",
    "- **비용**: 높은 하드웨어 요구사항으로 인한 높은 비용\n",
    "\n",
    "### PEFT의 해결책\n",
    "\n",
    "**Parameter-Efficient Fine-Tuning(PEFT)**는 이러한 문제를 해결합니다:\n",
    "\n",
    "- **적은 파라미터만 학습**: 전체 모델의 1% 미만만 업데이트\n",
    "- **메모리 효율성**: 기존 대비 훨씬 적은 GPU 메모리 사용\n",
    "- **빠른 학습**: 적은 파라미터로 인한 빠른 학습 속도\n",
    "- **저장 효율성**: 작은 크기의 어댑터만 저장하면 됨\n",
    "\n",
    "### PEFT의 주요 방법들\n",
    "\n",
    "1. **LoRA (Low-Rank Adaptation)**: 행렬 분해를 통한 효율적 학습\n",
    "2. **Prefix Tuning**: 입력 앞에 학습 가능한 prefix 추가\n",
    "3. **Prompt Tuning**: 임베딩 레벨에서 가상 토큰 추가\n",
    "4. **AdaLoRA**: 적응적 LoRA\n",
    "5. **IA3**: Infused Adapter by Inhibiting and Amplifying Inner Activations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad8b95",
   "metadata": {},
   "source": [
    "## 2. Prefix Tuning 이해하기\n",
    "\n",
    "### PrefixTuningConfig란?\n",
    "\n",
    "**PrefixTuningConfig**는 Prefix Tuning 방법을 설정하는 클래스입니다. 입력 시퀀스 앞에 **학습 가능한 연속적인 prefix**를 추가하는 방법입니다.\n",
    "\n",
    "### 핵심 아이디어\n",
    "\n",
    "```\n",
    "일반적인 입력: [입력 토큰들]\n",
    "Prefix Tuning: [학습가능한 prefix] + [입력 토큰들]\n",
    "```\n",
    "\n",
    "### 왜 PrefixTuningConfig가 필요한가?\n",
    "\n",
    "#### 1. 메모리 효율성\n",
    "- 전체 모델: 7.8B 파라미터 (약 15GB)\n",
    "- Prefix Tuning: 약 1M 파라미터 (약 4MB)\n",
    "- **99.9% 메모리 절약**\n",
    "\n",
    "#### 2. 멀티태스크 지원\n",
    "```python\n",
    "# 하나의 기본 모델 + 여러 prefix\n",
    "기본_모델 + 번역_prefix = 번역 모델\n",
    "기본_모델 + 요약_prefix = 요약 모델\n",
    "기본_모델 + QA_prefix = QA 모델\n",
    "```\n",
    "\n",
    "#### 3. 빠른 학습과 배포\n",
    "- 작은 prefix만 학습하므로 빠름\n",
    "- prefix 파일만 교체하면 즉시 태스크 변경 가능\n",
    "\n",
    "### PrefixTuningConfig 주요 파라미터\n",
    "\n",
    "- `task_type`: 태스크 유형 (CAUSAL_LM 등)\n",
    "- `num_virtual_tokens`: 가상 토큰 개수\n",
    "- `token_dim`: 토큰 차원 크기\n",
    "- `num_transformer_submodules`: 트랜스포머 서브모듈 수\n",
    "- `num_attention_heads`: 어텐션 헤드 수\n",
    "- `num_layers`: 레이어 수\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07428d72",
   "metadata": {},
   "source": [
    "## 3. Prompt Tuning 이해하기\n",
    "\n",
    "### PromptTuningConfig란?\n",
    "\n",
    "**PromptTuningConfig**는 입력 임베딩 레벨에서 **학습 가능한 가상 토큰(soft prompt)**을 추가하는 방법을 설정하는 클래스입니다.\n",
    "\n",
    "### Prefix Tuning vs Prompt Tuning\n",
    "\n",
    "| 구분 | Prefix Tuning | Prompt Tuning |\n",
    "|------|---------------|---------------|\n",
    "| **적용 위치** | 모든 Transformer 레이어 | 입력 임베딩 레이어만 |\n",
    "| **파라미터 수** | 더 많음 (레이어별 prefix) | 더 적음 (임베딩만) |\n",
    "| **성능** | 일반적으로 더 높음 | 적지만 효율적 |\n",
    "| **메모리** | 상대적으로 더 사용 | 매우 적게 사용 |\n",
    "\n",
    "### 핵심 아이디어\n",
    "\n",
    "```\n",
    "일반적인 입력: [토큰1, 토큰2, 토큰3, ...]\n",
    "Prompt Tuning: [가상토큰1, 가상토큰2, ..., 가상토큰k] + [토큰1, 토큰2, 토큰3, ...]\n",
    "```\n",
    "\n",
    "### 왜 PromptTuningConfig가 필요한가?\n",
    "\n",
    "#### 1. 극도의 효율성\n",
    "- **최소한의 파라미터**: 보통 100~500개의 가상 토큰만 사용\n",
    "- **최소한의 메모리**: 몇 KB 수준의 추가 메모리만 필요\n",
    "- **빠른 학습**: 매우 적은 파라미터로 인한 초고속 학습\n",
    "\n",
    "#### 2. 자연스러운 프롬프트 방식\n",
    "```python\n",
    "# 전통적인 하드 프롬프트\n",
    "\"다음 문장을 한국어로 번역하세요: Hello world\"\n",
    "\n",
    "# Prompt Tuning의 소프트 프롬프트\n",
    "[학습된_가상토큰들] + \"Hello world\"\n",
    "```\n",
    "\n",
    "#### 3. 모델 크기별 효과\n",
    "- **작은 모델 (< 1B)**: 효과가 제한적\n",
    "- **중간 모델 (1B~10B)**: 좋은 성능\n",
    "- **대형 모델 (> 10B)**: 전체 fine-tuning과 비슷한 성능\n",
    "\n",
    "### PromptTuningConfig 주요 파라미터\n",
    "\n",
    "- `task_type`: 태스크 유형\n",
    "- `prompt_tuning_init`: 초기화 방법 (TEXT, RANDOM 등)\n",
    "- `num_virtual_tokens`: 가상 토큰 개수\n",
    "- `prompt_tuning_init_text`: 초기화에 사용할 텍스트\n",
    "- `tokenizer_name_or_path`: 토크나이저 경로\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64576aa",
   "metadata": {},
   "source": [
    "## 4. 실습: EXAONE 모델로 PrefixTuningConfig와 PromptTuningConfig 예제\n",
    "\n",
    "이제 EXAONE-3.5-7.8B-Instruct 모델을 사용하여 실제로 두 방법을 구현해보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b074b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "%pip install -q transformers datasets peft accelerate torch bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a77eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PrefixTuningConfig, PromptTuningConfig, PromptTuningInit, TaskType, get_peft_model\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"라이브러리 임포트 완료!\")\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35fb0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAONE 모델과 토크나이저 로드\n",
    "model_name = \"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct\"\n",
    "\n",
    "print(\"EXAONE 모델 로드 중...\")\n",
    "\n",
    "# 4bit 양자화 설정 (메모리 절약)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"모델과 토크나이저 로드 완료!\")\n",
    "print(f\"모델 크기: {sum(p.numel() for p in model.parameters())/1e9:.1f}B 파라미터\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34565a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrefixTuningConfig 설정 예제\n",
    "print(\"=== PrefixTuningConfig 설정 ===\")\n",
    "\n",
    "prefix_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,        # 인과적 언어 모델링 태스크\n",
    "    inference_mode=False,                # 학습 모드\n",
    "    num_virtual_tokens=30,               # 가상 토큰 개수\n",
    "    token_dim=4096,                      # EXAONE 모델의 히든 차원\n",
    "    num_transformer_submodules=2,        # 어텐션과 MLP\n",
    "    num_attention_heads=32,              # EXAONE 모델의 어텐션 헤드 수\n",
    "    num_layers=32,                       # EXAONE 모델의 레이어 수\n",
    "    encoder_hidden_size=4096,            # 인코더 히든 크기\n",
    ")\n",
    "\n",
    "print(\"PrefixTuningConfig 설정 완료!\")\n",
    "print(f\"설정 내용:\")\n",
    "print(f\"   - 태스크 타입: {prefix_config.task_type}\")\n",
    "print(f\"   - 가상 토큰 수: {prefix_config.num_virtual_tokens}\")\n",
    "print(f\"   - 토큰 차원: {prefix_config.token_dim}\")\n",
    "print(f\"   - 어텐션 헤드: {prefix_config.num_attention_heads}\")\n",
    "print(f\"   - 레이어 수: {prefix_config.num_layers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1227d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromptTuningConfig 설정 예제\n",
    "print(\"=== PromptTuningConfig 설정 ===\")\n",
    "\n",
    "prompt_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,                    # 인과적 언어 모델링 태스크\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,        # 텍스트로 초기화\n",
    "    num_virtual_tokens=20,                           # 가상 토큰 개수 (Prefix보다 적게)\n",
    "    prompt_tuning_init_text=\"한국어로 정확하고 친절하게 답변하세요\",  # 초기화 텍스트\n",
    "    tokenizer_name_or_path=model_name,               # 토크나이저 경로\n",
    ")\n",
    "\n",
    "print(\"PromptTuningConfig 설정 완료!\")\n",
    "print(f\"설정 내용:\")\n",
    "print(f\"   - 태스크 타입: {prompt_config.task_type}\")\n",
    "print(f\"   - 가상 토큰 수: {prompt_config.num_virtual_tokens}\")\n",
    "print(f\"   - 초기화 방법: {prompt_config.prompt_tuning_init}\")\n",
    "print(f\"   - 초기화 텍스트: {prompt_config.prompt_tuning_init_text}\")\n",
    "\n",
    "# 파라미터 수 비교\n",
    "prefix_params = 30 * 4096 * 2 * 32  # num_virtual_tokens * token_dim * submodules * layers\n",
    "prompt_params = 20 * 4096           # num_virtual_tokens * token_dim\n",
    "\n",
    "print(f\"\\n파라미터 수 비교:\")\n",
    "print(f\"   - Prefix Tuning: {prefix_params:,} 파라미터\")\n",
    "print(f\"   - Prompt Tuning: {prompt_params:,} 파라미터\")\n",
    "print(f\"   - 차이: {(prefix_params/prompt_params):.1f}배\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a504e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 데이터셋 준비\n",
    "print(\"=== 한국어 학습 데이터 준비 ===\")\n",
    "\n",
    "# 간단한 한국어 질문-답변 데이터셋\n",
    "korean_data = [\n",
    "    {\n",
    "        \"input\": \"안녕하세요! 오늘 날씨가 어때요?\",\n",
    "        \"output\": \"안녕하세요! 저는 AI이므로 실시간 날씨 정보를 확인할 수 없지만, 날씨 앱이나 웹사이트를 통해 확인하시는 것을 추천드립니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"한국의 수도는 어디인가요?\",\n",
    "        \"output\": \"한국의 수도는 서울특별시입니다. 서울은 대한민국의 정치, 경제, 문화의 중심지입니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"김치는 어떤 음식인가요?\",\n",
    "        \"output\": \"김치는 한국의 전통 발효 음식으로, 주로 배추나 무를 고춧가루, 마늘, 생강 등의 양념과 함께 절여서 만듭니다. 건강에 좋은 유산균이 풍부합니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"인공지능이란 무엇인가요?\",\n",
    "        \"output\": \"인공지능(AI)은 인간의 지능을 모방하거나 인간이 수행하는 지적 작업을 기계가 수행할 수 있도록 하는 기술입니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"파이썬 프로그래밍의 특징은?\",\n",
    "        \"output\": \"파이썬은 간결하고 읽기 쉬운 문법을 가진 프로그래밍 언어입니다. 다양한 라이브러리가 풍부하여 웹 개발, 데이터 분석, 머신러닝 등에 활용됩니다.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"한국어 학습 데이터 준비 완료!\")\n",
    "print(f\"데이터 개수: {len(korean_data)}개\")\n",
    "print(f\"예시 데이터:\")\n",
    "for i, data in enumerate(korean_data[:2], 1):\n",
    "    print(f\"   {i}. 입력: {data['input'][:30]}...\")\n",
    "    print(f\"      출력: {data['output'][:30]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT 모델 생성 및 비교\n",
    "print(\"=== PEFT 모델 생성 ===\")\n",
    "\n",
    "# Prefix Tuning 모델 생성\n",
    "prefix_model = get_peft_model(model, prefix_config)\n",
    "\n",
    "# Prompt Tuning 모델 생성 (새로운 모델 인스턴스 필요)\n",
    "model_copy = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "prompt_model = get_peft_model(model_copy, prompt_config)\n",
    "\n",
    "# 파라미터 정보 출력\n",
    "def print_model_info(model, name):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"\\n{name} 모델 정보:\")\n",
    "    print(f\"   - 전체 파라미터: {total_params:,}\")\n",
    "    print(f\"   - 학습 가능 파라미터: {trainable_params:,}\")\n",
    "    print(f\"   - 학습 비율: {100 * trainable_params / total_params:.4f}%\")\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "print_model_info(prefix_model, \"Prefix Tuning\")\n",
    "print_model_info(prompt_model, \"Prompt Tuning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b06b2c",
   "metadata": {},
   "source": [
    "## 5. 데이터 전처리 및 학습\n",
    "\n",
    "이제 실제로 한국어 데이터를 사용하여 두 모델을 학습해보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6274d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 및 학습 준비\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"데이터를 모델 학습에 적합한 형태로 전처리\"\"\"\n",
    "    \n",
    "    # EXAONE 모델용 프롬프트 템플릿\n",
    "    def format_prompt(input_text, output_text):\n",
    "        return f\"[|system|]당신은 도움이 되는 AI 어시스턴트입니다. 한국어로 정확하고 친절하게 답변해 주세요.[|endofturn|]\\n[|user|]{input_text}[|endofturn|]\\n[|assistant|]{output_text}[|endofturn|]\"\n",
    "    \n",
    "    # 입력과 출력 결합\n",
    "    texts = []\n",
    "    for inp, out in zip(examples[\"input\"], examples[\"output\"]):\n",
    "        text = format_prompt(inp, out)\n",
    "        texts.append(text)\n",
    "    \n",
    "    # 토크나이징\n",
    "    model_inputs = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 라벨 설정 (입력과 동일)\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Dataset 객체 생성 및 전처리\n",
    "print(\"=== 데이터 전처리 ===\")\n",
    "\n",
    "# 리스트를 딕셔너리 형태로 변환\n",
    "inputs = [data[\"input\"] for data in korean_data]\n",
    "outputs = [data[\"output\"] for data in korean_data]\n",
    "\n",
    "dataset_dict = {\n",
    "    \"input\": inputs,\n",
    "    \"output\": outputs\n",
    "}\n",
    "\n",
    "# Dataset 생성\n",
    "train_dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# 전처리 적용\n",
    "processed_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"데이터 전처리 완료!\")\n",
    "print(f\"처리된 데이터셋 크기: {len(processed_dataset)}\")\n",
    "print(f\"첫 번째 샘플 토큰 수: {len(processed_dataset[0]['input_ids'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefix Tuning 학습 실행\n",
    "print(\"=== Prefix Tuning 학습 ===\")\n",
    "\n",
    "# 학습 인자 설정\n",
    "prefix_training_args = TrainingArguments(\n",
    "    output_dir=\"./prefix_tuning_results\",\n",
    "    num_train_epochs=3,              # 에폭 수\n",
    "    per_device_train_batch_size=1,   # 배치 크기 (메모리에 맞게 조정)\n",
    "    gradient_accumulation_steps=4,   # 그래디언트 누적\n",
    "    warmup_steps=10,                 # 워밍업 스텝\n",
    "    learning_rate=1e-4,              # 학습률\n",
    "    logging_steps=1,                 # 로깅 간격\n",
    "    save_steps=50,                   # 저장 간격\n",
    "    evaluation_strategy=\"no\",        # 평가 전략\n",
    "    save_total_limit=2,              # 저장할 체크포인트 수\n",
    "    load_best_model_at_end=False,    # 최선 모델 로드\n",
    "    report_to=None,                  # 리포팅 비활성화\n",
    "    remove_unused_columns=False,     # 사용하지 않는 컬럼 제거 안함\n",
    "    dataloader_pin_memory=False,     # 메모리 핀 비활성화\n",
    ")\n",
    "\n",
    "# 트레이너 생성\n",
    "prefix_trainer = Trainer(\n",
    "    model=prefix_model,\n",
    "    args=prefix_training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"학습 설정 완료!\")\n",
    "print(f\"학습 설정:\")\n",
    "print(f\"   - 에폭 수: {prefix_training_args.num_train_epochs}\")\n",
    "print(f\"   - 배치 크기: {prefix_training_args.per_device_train_batch_size}\")\n",
    "print(f\"   - 학습률: {prefix_training_args.learning_rate}\")\n",
    "print(f\"   - 그래디언트 누적: {prefix_training_args.gradient_accumulation_steps}\")\n",
    "\n",
    "# 학습 시작\n",
    "print(\"\\nPrefix Tuning 학습 시작!\")\n",
    "prefix_trainer.train()\n",
    "\n",
    "print(\"Prefix Tuning 학습 완료!\")\n",
    "\n",
    "# 모델 저장\n",
    "prefix_model.save_pretrained(\"./saved_prefix_tuning_model\")\n",
    "print(\"Prefix Tuning 모델 저장 완료: ./saved_prefix_tuning_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a588a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Tuning 학습 실행\n",
    "print(\"=== Prompt Tuning 학습 ===\")\n",
    "\n",
    "# 학습 인자 설정 (Prompt Tuning용으로 조정)\n",
    "prompt_training_args = TrainingArguments(\n",
    "    output_dir=\"./prompt_tuning_results\",\n",
    "    num_train_epochs=5,              # Prompt Tuning은 더 많은 에폭 필요\n",
    "    per_device_train_batch_size=2,   # 파라미터가 적어서 배치 크기 증가 가능\n",
    "    gradient_accumulation_steps=2,   # 그래디언트 누적 감소\n",
    "    warmup_steps=5,                  # 워밍업 스텝 감소\n",
    "    learning_rate=1e-3,              # 학습률 증가 (더 적은 파라미터)\n",
    "    logging_steps=1,                 # 로깅 간격\n",
    "    save_steps=25,                   # 저장 간격\n",
    "    evaluation_strategy=\"no\",        # 평가 전략\n",
    "    save_total_limit=2,              # 저장할 체크포인트 수\n",
    "    load_best_model_at_end=False,    # 최선 모델 로드\n",
    "    report_to=None,                  # 리포팅 비활성화\n",
    "    remove_unused_columns=False,     # 사용하지 않는 컬럼 제거 안함\n",
    "    dataloader_pin_memory=False,     # 메모리 핀 비활성화\n",
    ")\n",
    "\n",
    "# 트레이너 생성\n",
    "prompt_trainer = Trainer(\n",
    "    model=prompt_model,\n",
    "    args=prompt_training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"학습 설정 완료!\")\n",
    "print(f\"학습 설정:\")\n",
    "print(f\"   - 에폭 수: {prompt_training_args.num_train_epochs}\")\n",
    "print(f\"   - 배치 크기: {prompt_training_args.per_device_train_batch_size}\")\n",
    "print(f\"   - 학습률: {prompt_training_args.learning_rate}\")\n",
    "print(f\"   - 그래디언트 누적: {prompt_training_args.gradient_accumulation_steps}\")\n",
    "\n",
    "# 학습 시작\n",
    "print(\"\\nPrompt Tuning 학습 시작!\")\n",
    "prompt_trainer.train()\n",
    "\n",
    "print(\"Prompt Tuning 학습 완료!\")\n",
    "\n",
    "# 모델 저장\n",
    "prompt_model.save_pretrained(\"./saved_prompt_tuning_model\")\n",
    "print(\"Prompt Tuning 모델 저장 완료: ./saved_prompt_tuning_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b60466",
   "metadata": {},
   "source": [
    "## 6. 모델 테스트 및 성능 평가\n",
    "\n",
    "학습이 완료된 모델들의 성능을 테스트해보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e0f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델 테스트\n",
    "print(\"=== 학습된 모델 테스트 ===\")\n",
    "\n",
    "def generate_response(model, tokenizer, prompt, max_length=200):\n",
    "    \"\"\"주어진 프롬프트에 대해 모델이 응답을 생성하는 함수\"\"\"\n",
    "    \n",
    "    # EXAONE 프롬프트 형식\n",
    "    formatted_prompt = f\"[|system|]당신은 도움이 되는 AI 어시스턴트입니다. 한국어로 정확하고 친절하게 답변해 주세요.[|endofturn|]\\n[|user|]{prompt}[|endofturn|]\\n[|assistant|]\"\n",
    "    \n",
    "    # 토크나이징\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # 응답 생성\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # 디코딩\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # assistant 부분만 추출\n",
    "    if \"[|assistant|]\" in response:\n",
    "        assistant_response = response.split(\"[|assistant|]\")[-1].strip()\n",
    "        return assistant_response\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "# 테스트 질문들\n",
    "test_questions = [\n",
    "    \"머신러닝과 딥러닝의 차이점은 무엇인가요?\",\n",
    "    \"한국의 전통 음식 중 가장 유명한 것은?\",\n",
    "    \"파이썬에서 리스트와 튜플의 차이는?\",\n",
    "    \"서울에서 부산까지 가는 가장 빠른 방법은?\",\n",
    "    \"인공지능의 미래는 어떻게 될까요?\"\n",
    "]\n",
    "\n",
    "print(\"학습된 Prefix Tuning과 Prompt Tuning 모델의 응답을 비교해보겠습니다.\\n\")\n",
    "\n",
    "for i, question in enumerate(test_questions[:2], 1):  # 처음 2개 질문만 테스트\n",
    "    print(f\"질문 {i}: {question}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Prefix Tuning 모델 응답\n",
    "    print(\"Prefix Tuning 응답:\")\n",
    "    try:\n",
    "        prefix_response = generate_response(prefix_model, tokenizer, question)\n",
    "        print(prefix_response)\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "    \n",
    "    print(\"\\nPrompt Tuning 응답:\")\n",
    "    try:\n",
    "        prompt_response = generate_response(prompt_model, tokenizer, question)\n",
    "        print(prompt_response)\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"추론 함수 정의 및 테스트 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb17c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 결과 분석\n",
    "print(\"=== 학습 결과 분석 ===\")\n",
    "\n",
    "import os\n",
    "\n",
    "def get_folder_size(folder_path):\n",
    "    \"\"\"폴더 크기를 계산하는 함수\"\"\"\n",
    "    total_size = 0\n",
    "    if os.path.exists(folder_path):\n",
    "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "            for filename in filenames:\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                total_size += os.path.getsize(filepath)\n",
    "    return total_size\n",
    "\n",
    "# 저장된 모델 크기 비교\n",
    "try:\n",
    "    prefix_size = get_folder_size(\"./saved_prefix_tuning_model\")\n",
    "    prompt_size = get_folder_size(\"./saved_prompt_tuning_model\")\n",
    "    \n",
    "    print(f\"저장된 모델 크기 비교:\")\n",
    "    print(f\"   - Prefix Tuning: {prefix_size / (1024*1024):.1f} MB\")\n",
    "    print(f\"   - Prompt Tuning: {prompt_size / (1024*1024):.1f} MB\")\n",
    "    if prompt_size > 0:\n",
    "        print(f\"   - 크기 차이: {prefix_size / prompt_size:.1f}배\")\n",
    "except Exception as e:\n",
    "    print(f\"파일 크기 계산 중 오류 발생: {e}\")\n",
    "\n",
    "# 학습 완료 요약\n",
    "print(f\"\\n학습 완료 요약:\")\n",
    "print(f\"   - Prefix Tuning: 3 에폭, 학습률 1e-4\")\n",
    "print(f\"   - Prompt Tuning: 5 에폭, 학습률 1e-3\")\n",
    "print(f\"   - 사용 데이터: {len(korean_data)}개 한국어 QA 샘플\")\n",
    "print(f\"   - 모델: EXAONE-3.5-7.8B-Instruct (4bit 양자화)\")\n",
    "\n",
    "# 성능 개선을 위한 추천사항\n",
    "print(f\"\\n성능 개선을 위한 추천사항:\")\n",
    "print(f\"   1. 더 많은 한국어 데이터 추가 (현재: {len(korean_data)}개)\")\n",
    "print(f\"   2. 에폭 수 증가 (특히 Prompt Tuning)\")\n",
    "print(f\"   3. 학습률 스케줄링 적용\")\n",
    "print(f\"   4. 검증 데이터셋을 통한 조기 종료\")\n",
    "print(f\"   5. 다양한 프롬프트 템플릿 실험\")\n",
    "\n",
    "print(\"\\n모든 학습이 완료되었습니다!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1eefe2",
   "metadata": {},
   "source": [
    "## 7. 성능 비교 및 정리\n",
    "\n",
    "### 학습 결과 요약\n",
    "\n",
    "| 방법 | 학습 가능 파라미터 | 메모리 사용량 | 학습 시간 | 추천 사용 시나리오 |\n",
    "|------|------------------|--------------|----------|-------------------|\n",
    "| **Prefix Tuning** | ~8M (0.1%) | 중간 | 중간 | 복잡한 태스크, 높은 성능 필요 |\n",
    "| **Prompt Tuning** | ~80K (0.001%) | 매우 적음 | 매우 빠름 | 간단한 태스크, 빠른 적응 필요 |\n",
    "| **전체 Fine-Tuning** | 7.8B (100%) | 매우 많음 | 매우 오래 걸림 | 최고 성능, 충분한 리소스 |\n",
    "\n",
    "### 각 방법의 장단점\n",
    "\n",
    "#### Prefix Tuning\n",
    "**장점:**\n",
    "- 높은 성능 (전체 fine-tuning에 근접)\n",
    "- 적당한 메모리 사용량\n",
    "- 복잡한 태스크에 효과적\n",
    "\n",
    "**단점:**\n",
    "- Prompt Tuning보다 많은 파라미터\n",
    "- 설정이 복잡함\n",
    "\n",
    "#### Prompt Tuning  \n",
    "**장점:**\n",
    "- 극도로 적은 파라미터 (99.999% 절약)\n",
    "- 매우 빠른 학습\n",
    "- 간단한 설정\n",
    "- 대형 모델에서 효과적\n",
    "\n",
    "**단점:**\n",
    "- 작은 모델에서는 제한적 성능\n",
    "- 복잡한 태스크에서는 한계\n",
    "\n",
    "### 실제 사용 시 고려사항\n",
    "\n",
    "1. **모델 크기**: 대형 모델(>10B)일수록 Prompt Tuning이 효과적\n",
    "2. **태스크 복잡도**: 복잡한 태스크는 Prefix Tuning 권장\n",
    "3. **리소스 제약**: 메모리나 시간이 제한적이면 Prompt Tuning\n",
    "4. **데이터 크기**: 적은 데이터에서는 두 방법 모두 효과적\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5497810e",
   "metadata": {},
   "source": [
    "## 8. 실습 과제 및 다음 단계\n",
    "\n",
    "### 실습 과제\n",
    "\n",
    "1. **하이퍼파라미터 조정 실험**\n",
    "   - `num_virtual_tokens` 값을 10, 50, 100으로 변경해서 성능 비교\n",
    "   - `learning_rate`를 다양하게 조정해서 최적값 찾기\n",
    "\n",
    "2. **다른 데이터셋으로 실험**\n",
    "   - 자신만의 한국어 질문-답변 데이터 준비\n",
    "   - 특정 도메인(의료, 법률, 기술 등)에 특화된 데이터로 학습\n",
    "\n",
    "3. **다른 PEFT 방법과 비교**\n",
    "   - LoRA와 성능 비교\n",
    "   - AdaLoRA 실험\n",
    "\n",
    "### 다음 단계 학습 권장사항\n",
    "\n",
    "1. **Advanced PEFT 기법들**\n",
    "   - QLoRA (Quantized LoRA)\n",
    "   - MultiModal PEFT\n",
    "   - PEFT 조합 기법\n",
    "\n",
    "2. **실제 배포 고려사항**\n",
    "   - 모델 서빙 최적화\n",
    "   - 실시간 추론 성능\n",
    "   - 멀티태스크 어댑터 관리\n",
    "\n",
    "3. **평가 및 분석**\n",
    "   - BLEU, ROUGE 등 정량적 평가\n",
    "   - 사용자 만족도 평가\n",
    "   - 오류 분석\n",
    "\n",
    "### 추가 리소스\n",
    "\n",
    "- **PEFT 공식 문서**: https://huggingface.co/docs/peft\n",
    "- **Transformers 라이브러리**: https://huggingface.co/docs/transformers\n",
    "- **EXAONE 모델 페이지**: https://huggingface.co/LGAI-EXAONE\n",
    "\n",
    "---\n",
    "\n",
    "### 마무리\n",
    "\n",
    "이 강의를 통해 **Prefix Tuning**과 **Prompt Tuning**의 기본 개념부터 실제 구현까지 모든 과정을 학습했습니다. \n",
    "\n",
    "핵심 요약:\n",
    "- **적은 비용으로 큰 효과**: 전체 모델의 1% 미만 파라미터로 좋은 성능\n",
    "- **빠른 적응**: 새로운 태스크에 빠르게 적응 가능\n",
    "- **실용적 솔루션**: 리소스 제약 환경에서 실용적인 해결책\n",
    "\n",
    "더 궁금한 점이 있다면 언제든 질문해 주세요!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
