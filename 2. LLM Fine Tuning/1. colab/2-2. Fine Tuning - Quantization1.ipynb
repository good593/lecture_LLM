{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27fa4f50",
   "metadata": {},
   "source": [
    "# LLM Fine Tuning - Quantization ì…ë¬¸\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "1. Quantizationì˜ ê°œë…ê³¼ í•„ìš”ì„± ì´í•´\n",
    "2. Quantizationì˜ ì‘ë™ ì›ë¦¬ í•™ìŠµ\n",
    "3. EXAONE ëª¨ë¸ì„ í™œìš©í•œ ì‹¤ìŠµ\n",
    "4. í•œêµ­ì–´ ë°ì´í„°ì…‹ì„ ì´ìš©í•œ Fine Tuning ê²½í—˜\n",
    "\n",
    "---\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [Quantizationì´ë€ ë¬´ì—‡ì¸ê°€?](#1-quantizationì´ë€-ë¬´ì—‡ì¸ê°€)\n",
    "2. [ì™œ Quantizationì´ í•„ìš”í•œê°€?](#2-ì™œ-quantizationì´-í•„ìš”í•œê°€)\n",
    "3. [Quantizationì˜ ì¢…ë¥˜](#3-quantizationì˜-ì¢…ë¥˜)\n",
    "4. [ì‹¤ìŠµ: EXAONE ëª¨ë¸ Quantization](#4-ì‹¤ìŠµ-exaone-ëª¨ë¸-quantization)\n",
    "5. [í•œêµ­ì–´ ë°ì´í„°ì…‹ìœ¼ë¡œ Fine Tuning](#5-í•œêµ­ì–´-ë°ì´í„°ì…‹ìœ¼ë¡œ-fine-tuning)\n",
    "6. [ì„±ëŠ¥ ë¹„êµ ë° ë¶„ì„](#6-ì„±ëŠ¥-ë¹„êµ-ë°-ë¶„ì„)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ccc3cb",
   "metadata": {},
   "source": [
    "## 1. Quantizationì´ë€ ë¬´ì—‡ì¸ê°€?\n",
    "\n",
    "### ê¸°ë³¸ ê°œë…\n",
    "**Quantization(ì–‘ìí™”)**ì€ ì‹ ê²½ë§ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜(weights)ì™€ í™œì„±í™”(activations)ë¥¼ ë” ì ì€ ë¹„íŠ¸ë¡œ í‘œí˜„í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
    "\n",
    "### ê°„ë‹¨í•œ ì˜ˆì‹œ\n",
    "- **ì¼ë°˜ì ì¸ ê²½ìš°**: 32ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì  (FP32)\n",
    "  - ì˜ˆ: 3.14159265... â†’ 32ë¹„íŠ¸ë¡œ í‘œí˜„\n",
    "- **Quantization í›„**: 8ë¹„íŠ¸ ì •ìˆ˜ (INT8)  \n",
    "  - ì˜ˆ: 3.14159265... â†’ 3 (8ë¹„íŠ¸ë¡œ í‘œí˜„)\n",
    "\n",
    "### í•µì‹¬ ì•„ì´ë””ì–´\n",
    "```\n",
    "ë†’ì€ ì •ë°€ë„ (FP32) â†’ ë‚®ì€ ì •ë°€ë„ (INT8, INT4)\n",
    "```\n",
    "\n",
    "**ì¥ì **: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ, ì—°ì‚° ì†ë„ í–¥ìƒ  \n",
    "**ë‹¨ì **: ì •í™•ë„ ì•½ê°„ ì†ì‹¤ ê°€ëŠ¥\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccaaea4",
   "metadata": {},
   "source": [
    "## 2. ì™œ Quantizationì´ í•„ìš”í•œê°€?\n",
    "\n",
    "### í˜„ì‹¤ì ì¸ ë¬¸ì œë“¤\n",
    "\n",
    "#### ë¬¸ì œ 1: ë©”ëª¨ë¦¬ ë¶€ì¡±\n",
    "```\n",
    "7B ëª¨ë¸ (FP32) = ì•½ 28GB ë©”ëª¨ë¦¬ í•„ìš”\n",
    "7B ëª¨ë¸ (INT8) = ì•½ 7GB ë©”ëª¨ë¦¬ í•„ìš”  (4ë°° ì ˆì•½!)\n",
    "7B ëª¨ë¸ (INT4) = ì•½ 3.5GB ë©”ëª¨ë¦¬ í•„ìš” (8ë°° ì ˆì•½!)\n",
    "```\n",
    "\n",
    "#### ë¬¸ì œ 2: ëŠë¦° ì¶”ë¡  ì†ë„\n",
    "- GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ CPU ì‚¬ìš© â†’ ë§¤ìš° ëŠë¦¼\n",
    "- í° ëª¨ë¸ì¼ìˆ˜ë¡ ì—°ì‚°ëŸ‰ ì¦ê°€\n",
    "\n",
    "#### ë¬¸ì œ 3: ë¹„ìš© ë¬¸ì œ\n",
    "- í´ë¼ìš°ë“œ GPU ë¹„ìš© (A100 80GB: ì‹œê°„ë‹¹ $3-4)\n",
    "- ê°œì¸ìš© GPU êµ¬ë§¤ ë¹„ìš© (RTX 4090: ì•½ 200ë§Œì›)\n",
    "\n",
    "### Quantizationì˜ í•´ê²°ì±…\n",
    "\n",
    "#### ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±\n",
    "| ì •ë°€ë„ | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | 7B ëª¨ë¸ í¬ê¸° |\n",
    "|--------|---------------|---------------|\n",
    "| FP32   | 100%          | 28GB         |\n",
    "| FP16   | 50%           | 14GB         |\n",
    "| INT8   | 25%           | 7GB          |\n",
    "| INT4   | 12.5%         | 3.5GB        |\n",
    "\n",
    "#### ì†ë„ í–¥ìƒ\n",
    "- INT8 ì—°ì‚°ì´ FP32ë³´ë‹¤ 2-4ë°° ë¹ ë¦„\n",
    "- ë©”ëª¨ë¦¬ ëŒ€ì—­í­ íš¨ìœ¨ì„± ì¦ê°€\n",
    "- ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ í–¥ìƒ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb7d929",
   "metadata": {},
   "source": [
    "## 3. Quantizationì˜ ì¢…ë¥˜\n",
    "\n",
    "### 3.1 Post-Training Quantization (PTQ)\n",
    "- **ì„¤ëª…**: ì´ë¯¸ í›ˆë ¨ëœ ëª¨ë¸ì„ ë°”ë¡œ ì–‘ìí™”\n",
    "- **ì¥ì **: ë¹ ë¥´ê³  ê°„ë‹¨í•¨\n",
    "- **ë‹¨ì **: ì •í™•ë„ ì†ì‹¤ì´ í´ ìˆ˜ ìˆìŒ\n",
    "\n",
    "### 3.2 Quantization-Aware Training (QAT)  \n",
    "- **ì„¤ëª…**: í›ˆë ¨ ê³¼ì •ì—ì„œ ì–‘ìí™”ë¥¼ ê³ ë ¤\n",
    "- **ì¥ì **: ì •í™•ë„ ì†ì‹¤ ìµœì†Œí™”\n",
    "- **ë‹¨ì **: í›ˆë ¨ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼\n",
    "\n",
    "### 3.3 ì£¼ìš” Quantization ë°©ë²•ë“¤\n",
    "\n",
    "#### BitsAndBytes (8bit, 4bit)\n",
    "```python\n",
    "# 8bit ì–‘ìí™”\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True\n",
    ")\n",
    "\n",
    "# 4bit ì–‘ìí™”  \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "```\n",
    "\n",
    "#### GPTQ (Gradient-based Post-Training Quantization)\n",
    "- GPUì—ì„œ ë¹ ë¥¸ ì¶”ë¡ \n",
    "- 4bit ì–‘ìí™” ì§€ì›\n",
    "\n",
    "#### AWQ (Activation-aware Weight Quantization)\n",
    "- í™œì„±í™”ë¥¼ ê³ ë ¤í•œ ê°€ì¤‘ì¹˜ ì–‘ìí™”\n",
    "- ë” ë‚˜ì€ ì •í™•ë„ ìœ ì§€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7660ef",
   "metadata": {},
   "source": [
    "## 4. ì‹¤ìŠµ: EXAONE ëª¨ë¸ Quantization\n",
    "\n",
    "ì´ì œ ì‹¤ì œë¡œ LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct ëª¨ë¸ì„ ì‚¬ìš©í•´ì„œ Quantizationì„ ì‹¤ìŠµí•´ë³´ê² ìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06962d56",
   "metadata": {},
   "source": [
    "### 4.1 í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9fd62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "%pip install transformers accelerate bitsandbytes datasets torch\n",
    "%pip install peft trl\n",
    "%pip install sentencepiece protobuf\n",
    "\n",
    "# GPU ë©”ëª¨ë¦¬ í™•ì¸\n",
    "import torch\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU ì´ë¦„: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8b02c",
   "metadata": {},
   "source": [
    "### 4.2 EXAONE ëª¨ë¸ ë¡œë“œ (ì¼ë°˜ FP16 vs Quantized ë¹„êµ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5cdff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# ëª¨ë¸ ì´ë¦„\n",
    "model_name = \"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct\"\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "print(\"í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54c992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ í•¨ìˆ˜\n",
    "def get_memory_usage():\n",
    "    \"\"\"í˜„ì¬ GPUì™€ RAM ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ë°˜í™˜\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "        gpu_memory_cached = torch.cuda.memory_reserved() / 1024**3  # GB\n",
    "    else:\n",
    "        gpu_memory = 0\n",
    "        gpu_memory_cached = 0\n",
    "    \n",
    "    ram_memory = psutil.Process(os.getpid()).memory_info().rss / 1024**3  # GB\n",
    "    \n",
    "    return {\n",
    "        \"GPU ë©”ëª¨ë¦¬ (ì‚¬ìš©ì¤‘)\": f\"{gpu_memory:.2f} GB\",\n",
    "        \"GPU ë©”ëª¨ë¦¬ (ìºì‹œí¬í•¨)\": f\"{gpu_memory_cached:.2f} GB\", \n",
    "        \"RAM ë©”ëª¨ë¦¬\": f\"{ram_memory:.2f} GB\"\n",
    "    }\n",
    "\n",
    "# ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ\n",
    "print(\"ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "for key, value in get_memory_usage().items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd42801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 8bit Quantization ì„¤ì •\n",
    "print(\"=== 8bit Quantization ëª¨ë¸ ë¡œë“œ ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# BitsAndBytesConfig for 8bit\n",
    "bnb_config_8bit = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "try:\n",
    "    # 8bit ëª¨ë¸ ë¡œë“œ\n",
    "    model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config_8bit,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    load_time_8bit = time.time() - start_time\n",
    "    print(f\"8bit ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {load_time_8bit:.2f}ì´ˆ\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸\n",
    "    print(\"8bit ëª¨ë¸ ë¡œë“œ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "    for key, value in get_memory_usage().items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"8bit ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    model_8bit = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 4bit Quantization ì„¤ì • (ë” ê·¹ë‹¨ì ì¸ ì–‘ìí™”)\n",
    "print(\"\\n=== 4bit Quantization ëª¨ë¸ ë¡œë“œ ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# BitsAndBytesConfig for 4bit  \n",
    "bnb_config_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Normal Float 4bit\n",
    "    bnb_4bit_use_double_quant=True  # Double quantization\n",
    ")\n",
    "\n",
    "try:\n",
    "    # ê¸°ì¡´ ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ\n",
    "    if 'model_8bit' in locals() and model_8bit is not None:\n",
    "        del model_8bit\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # 4bit ëª¨ë¸ ë¡œë“œ\n",
    "    model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config_4bit,\n",
    "        device_map=\"auto\", \n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    load_time_4bit = time.time() - start_time\n",
    "    print(f\"4bit ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {load_time_4bit:.2f}ì´ˆ\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸\n",
    "    print(\"4bit ëª¨ë¸ ë¡œë“œ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "    for key, value in get_memory_usage().items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"4bit ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    model_4bit = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbfa9b7",
   "metadata": {},
   "source": [
    "### 4.3 Quantized ëª¨ë¸ í…ŒìŠ¤íŠ¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a77cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ìš© í”„ë¡¬í”„íŠ¸ (í•œêµ­ì–´)\n",
    "test_prompt = \"\"\"ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì§ˆë¬¸: ì¸ê³µì§€ëŠ¥ì´ ìš°ë¦¬ ìƒí™œì— ë¯¸ì¹˜ëŠ” ê¸ì •ì ì¸ ì˜í–¥ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ë‹µë³€:\"\"\"\n",
    "\n",
    "# í† í¬ë‚˜ì´ì§•\n",
    "def generate_response(model, prompt, max_length=200):\n",
    "    \"\"\"ëª¨ë¸ë¡œë¶€í„° ì‘ë‹µ ìƒì„±\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    \n",
    "    # ì¶”ë¡  ì‹œê°„ ì¸¡ì •\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    # ë””ì½”ë”©\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response, generation_time\n",
    "\n",
    "print(\"í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸:\")\n",
    "print(test_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356fd4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4bit ëª¨ë¸ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
    "if 'model_4bit' in locals() and model_4bit is not None:\n",
    "    print(\"=== 4bit Quantized ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ===\")\n",
    "    \n",
    "    response_4bit, time_4bit = generate_response(model_4bit, test_prompt)\n",
    "    \n",
    "    print(f\"ì¶”ë¡  ì‹œê°„: {time_4bit:.2f}ì´ˆ\")\n",
    "    print(f\"ì‘ë‹µ ê¸¸ì´: {len(response_4bit)}ì\")\n",
    "    print(\"\\nìƒì„±ëœ ì‘ë‹µ:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(response_4bit)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ \n",
    "    print(\"\\nì¶”ë¡  í›„ ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "    for key, value in get_memory_usage().items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"4bit ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c48b9b4",
   "metadata": {},
   "source": [
    "## 5. í•œêµ­ì–´ ë°ì´í„°ì…‹ìœ¼ë¡œ Fine Tuning\n",
    "\n",
    "ì´ì œ Quantized ëª¨ë¸ì„ í•œêµ­ì–´ ë°ì´í„°ì…‹ìœ¼ë¡œ Fine Tuning í•´ë³´ê² ìŠµë‹ˆë‹¤. PEFT(Parameter Efficient Fine Tuning) ê¸°ë²•ì¸ LoRAë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7175b11e",
   "metadata": {},
   "source": [
    "### 5.1 í•œêµ­ì–´ ë°ì´í„°ì…‹ ì¤€ë¹„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f5c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ ì§ˆë¬¸-ë‹µë³€ ë°ì´í„°ì…‹ ìƒì„±\n",
    "korean_dataset = [\n",
    "    {\n",
    "        \"instruction\": \"ì¸ê³µì§€ëŠ¥ì˜ ë°œì „ì´ ìš°ë¦¬ ì‚¬íšŒì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"ì¸ê³µì§€ëŠ¥ì˜ ë°œì „ì€ ìš°ë¦¬ ì‚¬íšŒì— ë‹¤ì–‘í•œ ê¸ì •ì ì¸ ë³€í™”ë¥¼ ê°€ì ¸ì˜¤ê³  ìˆìŠµë‹ˆë‹¤. ì˜ë£Œ ë¶„ì•¼ì—ì„œëŠ” ì •í™•í•œ ì§„ë‹¨ê³¼ ë§ì¶¤í˜• ì¹˜ë£Œê°€ ê°€ëŠ¥í•´ì§€ê³ , êµìœ¡ ë¶„ì•¼ì—ì„œëŠ” ê°œì¸í™”ëœ í•™ìŠµ ê²½í—˜ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ìë™í™”ë¥¼ í†µí•´ ì—…ë¬´ íš¨ìœ¨ì„±ì´ í¬ê²Œ í–¥ìƒë˜ê³  ìˆìŠµë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"ê¸°í›„ë³€í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê°œì¸ì˜ ì‹¤ì²œ ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"ê¸°í›„ë³€í™” ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ê°œì¸ì´ ì‹¤ì²œí•  ìˆ˜ ìˆëŠ” ë°©ì•ˆë“¤ì´ ìˆìŠµë‹ˆë‹¤. ì—ë„ˆì§€ ì ˆì•½ì„ ìœ„í•´ ë¶ˆí•„ìš”í•œ ì „ê¸° ì‚¬ìš©ì„ ì¤„ì´ê³ , ëŒ€ì¤‘êµí†µì´ë‚˜ ìì „ê±° ì´ìš©ì„ ëŠ˜ë¦¬ë©°, ì¬í™œìš©ì„ ì ê·¹ì ìœ¼ë¡œ ì‹¤ì²œí•˜ê³ , ì¹œí™˜ê²½ ì œí’ˆì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"ê±´ê°•í•œ ì‹ìŠµê´€ì˜ ì¤‘ìš”ì„±ê³¼ ì‹¤ì²œ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"ê±´ê°•í•œ ì‹ìŠµê´€ì€ ì§ˆë³‘ ì˜ˆë°©ê³¼ ì‚¶ì˜ ì§ˆ í–¥ìƒì— ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ê·œì¹™ì ì¸ ì‹ì‚¬ ì‹œê°„ì„ ì§€í‚¤ê³ , ë‹¤ì–‘í•œ ì˜ì–‘ì†Œë¥¼ ê· í˜• ìˆê²Œ ì„­ì·¨í•˜ë©°, ì¶©ë¶„í•œ ìˆ˜ë¶„ ì„­ì·¨ì™€ í•¨ê»˜ ê°€ê³µì‹í’ˆì„ ì¤„ì´ê³  ì‹ ì„ í•œ ì¬ë£Œë¡œ ë§Œë“  ìŒì‹ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"íš¨ê³¼ì ì¸ ì‹œê°„ ê´€ë¦¬ ë°©ë²•ì— ëŒ€í•´ ì¡°ì–¸í•´ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"íš¨ê³¼ì ì¸ ì‹œê°„ ê´€ë¦¬ë¥¼ ìœ„í•´ì„œëŠ” ìš°ì„ ìˆœìœ„ë¥¼ ëª…í™•íˆ ì •í•˜ê³ , í•  ì¼ ëª©ë¡ì„ ì‘ì„±í•˜ì—¬ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë˜í•œ ì§‘ì¤‘í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì„ ë§Œë“¤ê³ , ì ì ˆí•œ íœ´ì‹ì„ ì·¨í•˜ë©°, ë¶ˆí•„ìš”í•œ ì¼ë“¤ì„ ê³¼ê°íˆ ì¤„ì´ëŠ” ê²ƒë„ í•„ìš”í•©ë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"ë…ì„œì˜ ì¥ì ê³¼ ì˜¬ë°”ë¥¸ ë…ì„œ ìŠµê´€ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"ë…ì„œëŠ” ì§€ì‹ í™•ì¥, ì–´íœ˜ë ¥ í–¥ìƒ, ì°½ì˜ë ¥ ê°œë°œ, ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œ ë“± ë‹¤ì–‘í•œ ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ë…ì„œ ìŠµê´€ì„ ìœ„í•´ì„œëŠ” ë§¤ì¼ ì¼ì •í•œ ì‹œê°„ì„ ë…ì„œì— í• ì• í•˜ê³ , ë‹¤ì–‘í•œ ì¥ë¥´ì˜ ì±…ì„ ì½ìœ¼ë©°, ì½ì€ ë‚´ìš©ì— ëŒ€í•´ ìƒê°í•˜ê³  ê¸°ë¡í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"í•œêµ­ì–´ í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(f\"ì´ {len(korean_dataset)}ê°œì˜ ë°ì´í„°\")\n",
    "print(\"\\nì²« ë²ˆì§¸ ì˜ˆì‹œ:\")\n",
    "print(f\"ì§ˆë¬¸: {korean_dataset[0]['instruction']}\")\n",
    "print(f\"ë‹µë³€: {korean_dataset[0]['output']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed67f3",
   "metadata": {},
   "source": [
    "### 5.2 LoRA ì„¤ì • ë° Fine Tuning ì¤€ë¹„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c5181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "# LoRA ì„¤ì •\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                 # rank: LoRAì˜ ì°¨ì› (ì‘ì„ìˆ˜ë¡ ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "    lora_alpha=32,        # LoRA scaling parameter\n",
    "    target_modules=[      # LoRAë¥¼ ì ìš©í•  ë ˆì´ì–´ë“¤\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.1,     # dropout ë¹„ìœ¨\n",
    "    bias=\"none\",          # bias ì²˜ë¦¬ ë°©ì‹\n",
    "    task_type=\"CAUSAL_LM\" # íƒœìŠ¤í¬ íƒ€ì…\n",
    ")\n",
    "\n",
    "print(\"LoRA ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"LoRA rank: {lora_config.r}\")\n",
    "print(f\"Target modules: {lora_config.target_modules}\")\n",
    "\n",
    "# 4bit ëª¨ë¸ì„ LoRA í•™ìŠµìš©ìœ¼ë¡œ ì¤€ë¹„\n",
    "if 'model_4bit' in locals() and model_4bit is not None:\n",
    "    model_4bit = prepare_model_for_kbit_training(model_4bit)\n",
    "    model_4bit = get_peft_model(model_4bit, lora_config)\n",
    "    \n",
    "    # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸\n",
    "    model_4bit.print_trainable_parameters()\n",
    "    print(\"LoRA ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "else:\n",
    "    print(\"4bit ëª¨ë¸ì´ ì—†ì–´ì„œ LoRA ì„¤ì •ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aae7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def format_instruction(example):\n",
    "    \"\"\"Instruction í˜•íƒœë¡œ ë°ì´í„° í¬ë§·íŒ…\"\"\"\n",
    "    prompt = f\"\"\"ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì§ˆë¬¸: {example['instruction']}\n",
    "\n",
    "ë‹µë³€: {example['output']}\"\"\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# ë°ì´í„°ì…‹ì„ Hugging Face Datasetìœ¼ë¡œ ë³€í™˜\n",
    "train_dataset = Dataset.from_list(korean_dataset)\n",
    "train_dataset = train_dataset.map(format_instruction)\n",
    "\n",
    "print(\"ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "print(f\"í•™ìŠµ ë°ì´í„° ê°œìˆ˜: {len(train_dataset)}\")\n",
    "print(\"\\nì „ì²˜ë¦¬ëœ ì²« ë²ˆì§¸ ì˜ˆì‹œ:\")\n",
    "print(train_dataset[0]['text'][:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d799d",
   "metadata": {},
   "source": [
    "### 5.3 Fine Tuning ì‹¤í–‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f42dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì½œë ˆì´í„° í•¨ìˆ˜\n",
    "def data_collator(examples):\n",
    "    \"\"\"ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬\"\"\"\n",
    "    texts = [example['text'] for example in examples]\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì§•\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # labelsëŠ” input_idsì™€ ë™ì¼ (ì–¸ì–´ ëª¨ë¸ë§)\n",
    "    tokenized['labels'] = tokenized['input_ids'].clone()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# í•™ìŠµ ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./korean-exaone-lora\",      # ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "    num_train_epochs=1,                      # ì—í¬í¬ ìˆ˜ (ë°ëª¨ìš©ìœ¼ë¡œ 1ë¡œ ì„¤ì •)\n",
    "    per_device_train_batch_size=1,           # ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "    gradient_accumulation_steps=4,           # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…\n",
    "    learning_rate=2e-4,                      # í•™ìŠµë¥ \n",
    "    warmup_steps=10,                         # ì›Œë°ì—… ìŠ¤í…\n",
    "    logging_steps=1,                         # ë¡œê¹… ê°„ê²©\n",
    "    save_steps=50,                           # ì €ì¥ ê°„ê²©\n",
    "    evaluation_strategy=\"no\",                # í‰ê°€ ì „ëµ\n",
    "    save_total_limit=3,                      # ì €ì¥í•  ì²´í¬í¬ì¸íŠ¸ ìˆ˜\n",
    "    remove_unused_columns=False,             # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼ ì œê±°í•˜ì§€ ì•ŠìŒ\n",
    "    dataloader_pin_memory=False,             # ë©”ëª¨ë¦¬ í•€ ë¹„í™œì„±í™”\n",
    ")\n",
    "\n",
    "print(\"í•™ìŠµ ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"í•™ìŠµë¥ : {training_args.learning_rate}\")\n",
    "print(f\"ì—í¬í¬: {training_args.num_train_epochs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f48d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer ìƒì„± ë° í•™ìŠµ ì‹¤í–‰\n",
    "if 'model_4bit' in locals() and model_4bit is not None:\n",
    "    print(\"=== Fine Tuning ì‹œì‘ ===\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model_4bit,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # í•™ìŠµ ì‹œì‘ ì‹œê°„ ê¸°ë¡\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # í•™ìŠµ ì‹¤í–‰\n",
    "        trainer.train()\n",
    "        \n",
    "        training_time = time.time() - training_start_time\n",
    "        print(f\"\\ní•™ìŠµ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {training_time:.2f}ì´ˆ\")\n",
    "        \n",
    "        # ëª¨ë¸ ì €ì¥\n",
    "        trainer.save_model()\n",
    "        print(\"ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\")\n",
    "        \n",
    "        # í•™ìŠµ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ\n",
    "        print(\"\\ní•™ìŠµ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "        for key, value in get_memory_usage().items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        print(\"ì´ëŠ” ì •ìƒì ì¸ í˜„ìƒì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ë°ëª¨ í™˜ê²½ì˜ ë©”ëª¨ë¦¬ ì œí•œ)\")\n",
    "        \n",
    "else:\n",
    "    print(\"4bit ëª¨ë¸ì´ ì—†ì–´ì„œ Fine Tuningì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "    print(\"ìœ„ì˜ ì…€ë“¤ì„ ì‹¤í–‰í•´ì„œ ëª¨ë¸ì„ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87500b9b",
   "metadata": {},
   "source": [
    "## 6. ì„±ëŠ¥ ë¹„êµ ë° ë¶„ì„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be549c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tuning í›„ ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
    "test_questions = [\n",
    "    \"AI ê¸°ìˆ ì˜ ë¯¸ë˜ì— ëŒ€í•´ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?\",\n",
    "    \"í™˜ê²½ ë³´í˜¸ë¥¼ ìœ„í•´ ìš°ë¦¬ê°€ í•  ìˆ˜ ìˆëŠ” ì¼ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    \"íš¨ê³¼ì ì¸ í•™ìŠµ ë°©ë²•ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.\"\n",
    "]\n",
    "\n",
    "print(\"=== Fine Tuning í›„ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ===\")\n",
    "\n",
    "if 'model_4bit' in locals() and model_4bit is not None:\n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n{i}. ì§ˆë¬¸: {question}\")\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "        test_prompt = f\"\"\"ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ë‹µë³€:\"\"\"\n",
    "        \n",
    "        # ì‘ë‹µ ìƒì„±\n",
    "        response, gen_time = generate_response(model_4bit, test_prompt, max_length=150)\n",
    "        \n",
    "        # ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "        answer_start = response.find(\"ë‹µë³€:\") + 3\n",
    "        answer = response[answer_start:].strip()\n",
    "        \n",
    "        print(f\"ë‹µë³€: {answer}\")\n",
    "        print(f\"ìƒì„± ì‹œê°„: {gen_time:.2f}ì´ˆ\")\n",
    "        print(\"-\" * 50)\n",
    "else:\n",
    "    print(\"Fine Tuningëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef3748a",
   "metadata": {},
   "source": [
    "### 6.1 Quantization íš¨ê³¼ ì •ë¦¬\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a4bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization íš¨ê³¼ ìš”ì•½\n",
    "print(\"=== Quantization íš¨ê³¼ ë¶„ì„ ===\")\n",
    "print()\n",
    "\n",
    "# ì´ë¡ ì  ë©”ëª¨ë¦¬ ì ˆì•½ íš¨ê³¼\n",
    "model_size_fp16 = 7.8 * 2  # 7.8B parameters * 2 bytes (FP16)\n",
    "model_size_8bit = 7.8 * 1  # 7.8B parameters * 1 byte (INT8)\n",
    "model_size_4bit = 7.8 * 0.5  # 7.8B parameters * 0.5 bytes (INT4)\n",
    "\n",
    "print(\"ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ (ì´ë¡ ì ):\")\n",
    "print(f\"  FP16:  {model_size_fp16:.1f} GB\")\n",
    "print(f\"  8bit:  {model_size_8bit:.1f} GB ({model_size_fp16/model_size_8bit:.1f}ë°° ì ˆì•½)\")\n",
    "print(f\"  4bit:  {model_size_4bit:.1f} GB ({model_size_fp16/model_size_4bit:.1f}ë°° ì ˆì•½)\")\n",
    "print()\n",
    "\n",
    "print(\"âš¡ ì¶”ë¡  ì†ë„:\")\n",
    "print(\"  - 8bit: FP16 ëŒ€ë¹„ 1.5-2ë°° ë¹ ë¦„\")\n",
    "print(\"  - 4bit: FP16 ëŒ€ë¹„ 2-3ë°° ë¹ ë¦„\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ¯ ì •í™•ë„:\")\n",
    "print(\"  - 8bit: ì›ë³¸ ëŒ€ë¹„ 95-98% ìœ ì§€\")\n",
    "print(\"  - 4bit: ì›ë³¸ ëŒ€ë¹„ 90-95% ìœ ì§€\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ’° ë¹„ìš© ì ˆì•½:\")\n",
    "print(\"  - GPU ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ ê°ì†Œ\")\n",
    "print(\"  - í´ë¼ìš°ë“œ ë¹„ìš© ì ˆì•½ (ë” ì‘ì€ GPU ì‚¬ìš© ê°€ëŠ¥)\")\n",
    "print(\"  - ì¶”ë¡  ì†ë„ í–¥ìƒìœ¼ë¡œ ì²˜ë¦¬ëŸ‰ ì¦ê°€\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea89af6",
   "metadata": {},
   "source": [
    "## 7. í•™ìŠµ ì •ë¦¬ ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### 7.1 ì˜¤ëŠ˜ ë°°ìš´ ë‚´ìš©\n",
    "\n",
    "1. **Quantization ê°œë…**\n",
    "   - ì‹ ê²½ë§ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë” ì ì€ ë¹„íŠ¸ë¡œ í‘œí˜„í•˜ëŠ” ê¸°ìˆ \n",
    "   - FP32 â†’ FP16 â†’ INT8 â†’ INT4 ìˆœìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "\n",
    "2. **Quantizationì˜ í•„ìš”ì„±**\n",
    "   - ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ í•´ê²°\n",
    "   - ì¶”ë¡  ì†ë„ í–¥ìƒ\n",
    "   - ë¹„ìš© ì ˆì•½\n",
    "\n",
    "3. **ì‹¤ìŠµ ë‚´ìš©**\n",
    "   - EXAONE 7.8B ëª¨ë¸ì„ 4bitë¡œ ì–‘ìí™”\n",
    "   - í•œêµ­ì–´ ë°ì´í„°ì…‹ìœ¼ë¡œ LoRA Fine Tuning\n",
    "   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "### 7.2 ë‹¤ìŒ ë‹¨ê³„ í•™ìŠµ ë°©í–¥\n",
    "\n",
    "1. **ê³ ê¸‰ Quantization ê¸°ë²•**\n",
    "   - GPTQ, AWQ ë“± ë‹¤ë¥¸ ì–‘ìí™” ë°©ë²•\n",
    "   - Mixed-precision í•™ìŠµ\n",
    "\n",
    "2. **ë” ë³µì¡í•œ Fine Tuning**\n",
    "   - ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ í™œìš©\n",
    "   - ë‹¤ì–‘í•œ PEFT ê¸°ë²• (AdaLoRA, QLoRA ë“±)\n",
    "\n",
    "3. **ëª¨ë¸ ë°°í¬**\n",
    "   - Quantized ëª¨ë¸ì˜ ì‹¤ì œ ì„œë¹„ìŠ¤ ì ìš©\n",
    "   - ONNX, TensorRT ë“± ìµœì í™” ë„êµ¬ í™œìš©\n",
    "\n",
    "### 7.3 ì¶”ì²œ ìë£Œ\n",
    "\n",
    "- Hugging Face Transformers ë¬¸ì„œ\n",
    "- PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ íŠœí† ë¦¬ì–¼\n",
    "- BitsAndBytes ê³µì‹ ë¬¸ì„œ\n",
    "- ìµœì‹  Quantization ì—°êµ¬ ë…¼ë¬¸ë“¤\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
