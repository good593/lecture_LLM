{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27fa4f50",
   "metadata": {},
   "source": [
    "# LLM Fine Tuning - Quantization 입문\n",
    "\n",
    "## 학습 목표\n",
    "1. Quantization의 개념과 필요성 이해\n",
    "2. Quantization의 작동 원리 학습\n",
    "3. EXAONE 모델을 활용한 실습\n",
    "4. 한국어 데이터셋을 이용한 Fine Tuning 경험\n",
    "\n",
    "---\n",
    "\n",
    "## 목차\n",
    "1. [Quantization이란 무엇인가?](#1-quantization이란-무엇인가)\n",
    "2. [왜 Quantization이 필요한가?](#2-왜-quantization이-필요한가)\n",
    "3. [Quantization의 종류](#3-quantization의-종류)\n",
    "4. [실습: EXAONE 모델 Quantization](#4-실습-exaone-모델-quantization)\n",
    "5. [한국어 데이터셋으로 Fine Tuning](#5-한국어-데이터셋으로-fine-tuning)\n",
    "6. [성능 비교 및 분석](#6-성능-비교-및-분석)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ccc3cb",
   "metadata": {},
   "source": [
    "## 1. Quantization이란 무엇인가?\n",
    "\n",
    "### 기본 개념\n",
    "**Quantization(양자화)**은 신경망 모델의 가중치(weights)와 활성화(activations)를 더 적은 비트로 표현하는 기술입니다.\n",
    "\n",
    "### 간단한 예시\n",
    "- **일반적인 경우**: 32비트 부동소수점 (FP32)\n",
    "  - 예: 3.14159265... → 32비트로 표현\n",
    "- **Quantization 후**: 8비트 정수 (INT8)  \n",
    "  - 예: 3.14159265... → 3 (8비트로 표현)\n",
    "\n",
    "### 핵심 아이디어\n",
    "```\n",
    "높은 정밀도 (FP32) → 낮은 정밀도 (INT8, INT4)\n",
    "```\n",
    "\n",
    "**장점**: 메모리 사용량 감소, 연산 속도 향상  \n",
    "**단점**: 정확도 약간 손실 가능\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccaaea4",
   "metadata": {},
   "source": [
    "## 2. 왜 Quantization이 필요한가?\n",
    "\n",
    "### 현실적인 문제들\n",
    "\n",
    "#### 문제 1: 메모리 부족\n",
    "```\n",
    "7B 모델 (FP32) = 약 28GB 메모리 필요\n",
    "7B 모델 (INT8) = 약 7GB 메모리 필요  (4배 절약!)\n",
    "7B 모델 (INT4) = 약 3.5GB 메모리 필요 (8배 절약!)\n",
    "```\n",
    "\n",
    "#### 문제 2: 느린 추론 속도\n",
    "- GPU 메모리 부족으로 CPU 사용 → 매우 느림\n",
    "- 큰 모델일수록 연산량 증가\n",
    "\n",
    "#### 문제 3: 비용 문제\n",
    "- 클라우드 GPU 비용 (A100 80GB: 시간당 $3-4)\n",
    "- 개인용 GPU 구매 비용 (RTX 4090: 약 200만원)\n",
    "\n",
    "### Quantization의 해결책\n",
    "\n",
    "#### 메모리 효율성\n",
    "| 정밀도 | 메모리 사용량 | 7B 모델 크기 |\n",
    "|--------|---------------|---------------|\n",
    "| FP32   | 100%          | 28GB         |\n",
    "| FP16   | 50%           | 14GB         |\n",
    "| INT8   | 25%           | 7GB          |\n",
    "| INT4   | 12.5%         | 3.5GB        |\n",
    "\n",
    "#### 속도 향상\n",
    "- INT8 연산이 FP32보다 2-4배 빠름\n",
    "- 메모리 대역폭 효율성 증가\n",
    "- 배치 처리 성능 향상\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb7d929",
   "metadata": {},
   "source": [
    "## 3. Quantization의 종류\n",
    "\n",
    "### 3.1 Post-Training Quantization (PTQ)\n",
    "- **설명**: 이미 훈련된 모델을 바로 양자화\n",
    "- **장점**: 빠르고 간단함\n",
    "- **단점**: 정확도 손실이 클 수 있음\n",
    "\n",
    "### 3.2 Quantization-Aware Training (QAT)  \n",
    "- **설명**: 훈련 과정에서 양자화를 고려\n",
    "- **장점**: 정확도 손실 최소화\n",
    "- **단점**: 훈련 시간이 오래 걸림\n",
    "\n",
    "### 3.3 주요 Quantization 방법들\n",
    "\n",
    "#### BitsAndBytes (8bit, 4bit)\n",
    "```python\n",
    "# 8bit 양자화\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True\n",
    ")\n",
    "\n",
    "# 4bit 양자화  \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "```\n",
    "\n",
    "#### GPTQ (Gradient-based Post-Training Quantization)\n",
    "- GPU에서 빠른 추론\n",
    "- 4bit 양자화 지원\n",
    "\n",
    "#### AWQ (Activation-aware Weight Quantization)\n",
    "- 활성화를 고려한 가중치 양자화\n",
    "- 더 나은 정확도 유지\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7660ef",
   "metadata": {},
   "source": [
    "## 4. 실습: EXAONE 모델 Quantization\n",
    "\n",
    "이제 실제로 LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct 모델을 사용해서 Quantization을 실습해보겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06962d56",
   "metadata": {},
   "source": [
    "### 4.1 환경 설정 및 라이브러리 설치\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9fd62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "%pip install transformers accelerate bitsandbytes datasets torch\n",
    "%pip install peft trl\n",
    "%pip install sentencepiece protobuf\n",
    "\n",
    "# GPU 메모리 확인\n",
    "import torch\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 이름: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8b02c",
   "metadata": {},
   "source": [
    "### 4.2 EXAONE 모델 로드 (일반 FP16 vs Quantized 비교)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5cdff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# 모델 이름\n",
    "model_name = \"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct\"\n",
    "\n",
    "# 토크나이저 로드\n",
    "print(\"토크나이저 로드 중...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"토크나이저 로드 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54c992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 사용량 확인 함수\n",
    "def get_memory_usage():\n",
    "    \"\"\"현재 GPU와 RAM 메모리 사용량을 반환\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "        gpu_memory_cached = torch.cuda.memory_reserved() / 1024**3  # GB\n",
    "    else:\n",
    "        gpu_memory = 0\n",
    "        gpu_memory_cached = 0\n",
    "    \n",
    "    ram_memory = psutil.Process(os.getpid()).memory_info().rss / 1024**3  # GB\n",
    "    \n",
    "    return {\n",
    "        \"GPU 메모리 (사용중)\": f\"{gpu_memory:.2f} GB\",\n",
    "        \"GPU 메모리 (캐시포함)\": f\"{gpu_memory_cached:.2f} GB\", \n",
    "        \"RAM 메모리\": f\"{ram_memory:.2f} GB\"\n",
    "    }\n",
    "\n",
    "# 초기 메모리 상태\n",
    "print(\"초기 메모리 상태:\")\n",
    "for key, value in get_memory_usage().items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd42801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 8bit Quantization 설정\n",
    "print(\"=== 8bit Quantization 모델 로드 ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# BitsAndBytesConfig for 8bit\n",
    "bnb_config_8bit = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "try:\n",
    "    # 8bit 모델 로드\n",
    "    model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config_8bit,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    load_time_8bit = time.time() - start_time\n",
    "    print(f\"8bit 모델 로드 완료! 소요시간: {load_time_8bit:.2f}초\")\n",
    "    \n",
    "    # 메모리 사용량 확인\n",
    "    print(\"8bit 모델 로드 후 메모리 상태:\")\n",
    "    for key, value in get_memory_usage().items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"8bit 모델 로드 실패: {e}\")\n",
    "    model_8bit = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 4bit Quantization 설정 (더 극단적인 양자화)\n",
    "print(\"\\n=== 4bit Quantization 모델 로드 ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# BitsAndBytesConfig for 4bit  \n",
    "bnb_config_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Normal Float 4bit\n",
    "    bnb_4bit_use_double_quant=True  # Double quantization\n",
    ")\n",
    "\n",
    "try:\n",
    "    # 기존 모델 메모리 해제\n",
    "    if 'model_8bit' in locals() and model_8bit is not None:\n",
    "        del model_8bit\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # 4bit 모델 로드\n",
    "    model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config_4bit,\n",
    "        device_map=\"auto\", \n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    load_time_4bit = time.time() - start_time\n",
    "    print(f\"4bit 모델 로드 완료! 소요시간: {load_time_4bit:.2f}초\")\n",
    "    \n",
    "    # 메모리 사용량 확인\n",
    "    print(\"4bit 모델 로드 후 메모리 상태:\")\n",
    "    for key, value in get_memory_usage().items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"4bit 모델 로드 실패: {e}\")\n",
    "    model_4bit = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbfa9b7",
   "metadata": {},
   "source": [
    "### 4.3 Quantized 모델 테스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a77cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트용 프롬프트 (한국어)\n",
    "test_prompt = \"\"\"아래는 사용자의 질문에 친절하고 정확하게 답변하는 AI 어시스턴트입니다.\n",
    "\n",
    "질문: 인공지능이 우리 생활에 미치는 긍정적인 영향에 대해 설명해주세요.\n",
    "\n",
    "답변:\"\"\"\n",
    "\n",
    "# 토크나이징\n",
    "def generate_response(model, prompt, max_length=200):\n",
    "    \"\"\"모델로부터 응답 생성\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    \n",
    "    # 추론 시간 측정\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    # 디코딩\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response, generation_time\n",
    "\n",
    "print(\"테스트 프롬프트:\")\n",
    "print(test_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356fd4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4bit 모델로 추론 테스트\n",
    "if 'model_4bit' in locals() and model_4bit is not None:\n",
    "    print(\"=== 4bit Quantized 모델 추론 테스트 ===\")\n",
    "    \n",
    "    response_4bit, time_4bit = generate_response(model_4bit, test_prompt)\n",
    "    \n",
    "    print(f\"추론 시간: {time_4bit:.2f}초\")\n",
    "    print(f\"응답 길이: {len(response_4bit)}자\")\n",
    "    print(\"\\n생성된 응답:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(response_4bit)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 메모리 사용량 \n",
    "    print(\"\\n추론 후 메모리 상태:\")\n",
    "    for key, value in get_memory_usage().items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"4bit 모델이 로드되지 않았습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c48b9b4",
   "metadata": {},
   "source": [
    "## 5. 한국어 데이터셋으로 Fine Tuning\n",
    "\n",
    "이제 Quantized 모델을 한국어 데이터셋으로 Fine Tuning 해보겠습니다. PEFT(Parameter Efficient Fine Tuning) 기법인 LoRA를 사용하겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7175b11e",
   "metadata": {},
   "source": [
    "### 5.1 한국어 데이터셋 준비\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f5c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 질문-답변 데이터셋 생성\n",
    "korean_dataset = [\n",
    "    {\n",
    "        \"instruction\": \"인공지능의 발전이 우리 사회에 미치는 영향에 대해 설명해주세요.\",\n",
    "        \"output\": \"인공지능의 발전은 우리 사회에 다양한 긍정적인 변화를 가져오고 있습니다. 의료 분야에서는 정확한 진단과 맞춤형 치료가 가능해지고, 교육 분야에서는 개인화된 학습 경험을 제공할 수 있습니다. 또한 자동화를 통해 업무 효율성이 크게 향상되고 있습니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"기후변화 문제를 해결하기 위한 개인의 실천 방안을 제시해주세요.\",\n",
    "        \"output\": \"기후변화 문제 해결을 위해 개인이 실천할 수 있는 방안들이 있습니다. 에너지 절약을 위해 불필요한 전기 사용을 줄이고, 대중교통이나 자전거 이용을 늘리며, 재활용을 적극적으로 실천하고, 친환경 제품을 선택하는 것이 중요합니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"건강한 식습관의 중요성과 실천 방법을 알려주세요.\",\n",
    "        \"output\": \"건강한 식습관은 질병 예방과 삶의 질 향상에 매우 중요합니다. 규칙적인 식사 시간을 지키고, 다양한 영양소를 균형 있게 섭취하며, 충분한 수분 섭취와 함께 가공식품을 줄이고 신선한 재료로 만든 음식을 선택하는 것이 좋습니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"효과적인 시간 관리 방법에 대해 조언해주세요.\",\n",
    "        \"output\": \"효과적인 시간 관리를 위해서는 우선순위를 명확히 정하고, 할 일 목록을 작성하여 체계적으로 관리하는 것이 중요합니다. 또한 집중할 수 있는 환경을 만들고, 적절한 휴식을 취하며, 불필요한 일들을 과감히 줄이는 것도 필요합니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"독서의 장점과 올바른 독서 습관에 대해 설명해주세요.\",\n",
    "        \"output\": \"독서는 지식 확장, 어휘력 향상, 창의력 개발, 스트레스 해소 등 다양한 장점이 있습니다. 올바른 독서 습관을 위해서는 매일 일정한 시간을 독서에 할애하고, 다양한 장르의 책을 읽으며, 읽은 내용에 대해 생각하고 기록하는 것이 좋습니다.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"한국어 학습 데이터셋 준비 완료!\")\n",
    "print(f\"총 {len(korean_dataset)}개의 데이터\")\n",
    "print(\"\\n첫 번째 예시:\")\n",
    "print(f\"질문: {korean_dataset[0]['instruction']}\")\n",
    "print(f\"답변: {korean_dataset[0]['output']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed67f3",
   "metadata": {},
   "source": [
    "### 5.2 LoRA 설정 및 Fine Tuning 준비\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c5181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                 # rank: LoRA의 차원 (작을수록 메모리 절약)\n",
    "    lora_alpha=32,        # LoRA scaling parameter\n",
    "    target_modules=[      # LoRA를 적용할 레이어들\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.1,     # dropout 비율\n",
    "    bias=\"none\",          # bias 처리 방식\n",
    "    task_type=\"CAUSAL_LM\" # 태스크 타입\n",
    ")\n",
    "\n",
    "print(\"LoRA 설정 완료!\")\n",
    "print(f\"LoRA rank: {lora_config.r}\")\n",
    "print(f\"Target modules: {lora_config.target_modules}\")\n",
    "\n",
    "# 4bit 모델을 LoRA 학습용으로 준비\n",
    "if 'model_4bit' in locals() and model_4bit is not None:\n",
    "    model_4bit = prepare_model_for_kbit_training(model_4bit)\n",
    "    model_4bit = get_peft_model(model_4bit, lora_config)\n",
    "    \n",
    "    # 학습 가능한 파라미터 확인\n",
    "    model_4bit.print_trainable_parameters()\n",
    "    print(\"LoRA 모델 준비 완료!\")\n",
    "else:\n",
    "    print(\"4bit 모델이 없어서 LoRA 설정을 건너뜁니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aae7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "def format_instruction(example):\n",
    "    \"\"\"Instruction 형태로 데이터 포맷팅\"\"\"\n",
    "    prompt = f\"\"\"아래는 사용자의 질문에 친절하고 정확하게 답변하는 AI 어시스턴트입니다.\n",
    "\n",
    "질문: {example['instruction']}\n",
    "\n",
    "답변: {example['output']}\"\"\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# 데이터셋을 Hugging Face Dataset으로 변환\n",
    "train_dataset = Dataset.from_list(korean_dataset)\n",
    "train_dataset = train_dataset.map(format_instruction)\n",
    "\n",
    "print(\"데이터 전처리 완료!\")\n",
    "print(f\"학습 데이터 개수: {len(train_dataset)}\")\n",
    "print(\"\\n전처리된 첫 번째 예시:\")\n",
    "print(train_dataset[0]['text'][:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d799d",
   "metadata": {},
   "source": [
    "### 5.3 Fine Tuning 실행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f42dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 콜레이터 함수\n",
    "def data_collator(examples):\n",
    "    \"\"\"배치 데이터 처리\"\"\"\n",
    "    texts = [example['text'] for example in examples]\n",
    "    \n",
    "    # 토크나이징\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # labels는 input_ids와 동일 (언어 모델링)\n",
    "    tokenized['labels'] = tokenized['input_ids'].clone()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./korean-exaone-lora\",      # 저장 디렉토리\n",
    "    num_train_epochs=1,                      # 에포크 수 (데모용으로 1로 설정)\n",
    "    per_device_train_batch_size=1,           # 배치 크기 (메모리 절약)\n",
    "    gradient_accumulation_steps=4,           # 그래디언트 누적 스텝\n",
    "    learning_rate=2e-4,                      # 학습률\n",
    "    warmup_steps=10,                         # 워밍업 스텝\n",
    "    logging_steps=1,                         # 로깅 간격\n",
    "    save_steps=50,                           # 저장 간격\n",
    "    evaluation_strategy=\"no\",                # 평가 전략\n",
    "    save_total_limit=3,                      # 저장할 체크포인트 수\n",
    "    remove_unused_columns=False,             # 사용하지 않는 컬럼 제거하지 않음\n",
    "    dataloader_pin_memory=False,             # 메모리 핀 비활성화\n",
    ")\n",
    "\n",
    "print(\"학습 설정 완료!\")\n",
    "print(f\"배치 크기: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"학습률: {training_args.learning_rate}\")\n",
    "print(f\"에포크: {training_args.num_train_epochs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f48d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 생성 및 학습 실행\n",
    "if 'model_4bit' in locals() and model_4bit is not None:\n",
    "    print(\"=== Fine Tuning 시작 ===\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model_4bit,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # 학습 시작 시간 기록\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # 학습 실행\n",
    "        trainer.train()\n",
    "        \n",
    "        training_time = time.time() - training_start_time\n",
    "        print(f\"\\n학습 완료! 소요시간: {training_time:.2f}초\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model()\n",
    "        print(\"모델 저장 완료!\")\n",
    "        \n",
    "        # 학습 후 메모리 상태\n",
    "        print(\"\\n학습 후 메모리 상태:\")\n",
    "        for key, value in get_memory_usage().items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"학습 중 오류 발생: {e}\")\n",
    "        print(\"이는 정상적인 현상일 수 있습니다. (데모 환경의 메모리 제한)\")\n",
    "        \n",
    "else:\n",
    "    print(\"4bit 모델이 없어서 Fine Tuning을 건너뜁니다.\")\n",
    "    print(\"위의 셀들을 실행해서 모델을 먼저 로드해주세요.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87500b9b",
   "metadata": {},
   "source": [
    "## 6. 성능 비교 및 분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be549c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tuning 후 모델 테스트\n",
    "test_questions = [\n",
    "    \"AI 기술의 미래에 대해 어떻게 생각하시나요?\",\n",
    "    \"환경 보호를 위해 우리가 할 수 있는 일은 무엇인가요?\",\n",
    "    \"효과적인 학습 방법을 추천해주세요.\"\n",
    "]\n",
    "\n",
    "print(\"=== Fine Tuning 후 모델 성능 테스트 ===\")\n",
    "\n",
    "if 'model_4bit' in locals() and model_4bit is not None:\n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n{i}. 질문: {question}\")\n",
    "        \n",
    "        # 프롬프트 구성\n",
    "        test_prompt = f\"\"\"아래는 사용자의 질문에 친절하고 정확하게 답변하는 AI 어시스턴트입니다.\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변:\"\"\"\n",
    "        \n",
    "        # 응답 생성\n",
    "        response, gen_time = generate_response(model_4bit, test_prompt, max_length=150)\n",
    "        \n",
    "        # 답변 부분만 추출\n",
    "        answer_start = response.find(\"답변:\") + 3\n",
    "        answer = response[answer_start:].strip()\n",
    "        \n",
    "        print(f\"답변: {answer}\")\n",
    "        print(f\"생성 시간: {gen_time:.2f}초\")\n",
    "        print(\"-\" * 50)\n",
    "else:\n",
    "    print(\"Fine Tuning된 모델이 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef3748a",
   "metadata": {},
   "source": [
    "### 6.1 Quantization 효과 정리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a4bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization 효과 요약\n",
    "print(\"=== Quantization 효과 분석 ===\")\n",
    "print()\n",
    "\n",
    "# 이론적 메모리 절약 효과\n",
    "model_size_fp16 = 7.8 * 2  # 7.8B parameters * 2 bytes (FP16)\n",
    "model_size_8bit = 7.8 * 1  # 7.8B parameters * 1 byte (INT8)\n",
    "model_size_4bit = 7.8 * 0.5  # 7.8B parameters * 0.5 bytes (INT4)\n",
    "\n",
    "print(\"📊 메모리 사용량 비교 (이론적):\")\n",
    "print(f\"  FP16:  {model_size_fp16:.1f} GB\")\n",
    "print(f\"  8bit:  {model_size_8bit:.1f} GB ({model_size_fp16/model_size_8bit:.1f}배 절약)\")\n",
    "print(f\"  4bit:  {model_size_4bit:.1f} GB ({model_size_fp16/model_size_4bit:.1f}배 절약)\")\n",
    "print()\n",
    "\n",
    "print(\"⚡ 추론 속도:\")\n",
    "print(\"  - 8bit: FP16 대비 1.5-2배 빠름\")\n",
    "print(\"  - 4bit: FP16 대비 2-3배 빠름\")\n",
    "print()\n",
    "\n",
    "print(\"🎯 정확도:\")\n",
    "print(\"  - 8bit: 원본 대비 95-98% 유지\")\n",
    "print(\"  - 4bit: 원본 대비 90-95% 유지\")\n",
    "print()\n",
    "\n",
    "print(\"💰 비용 절약:\")\n",
    "print(\"  - GPU 메모리 요구량 감소\")\n",
    "print(\"  - 클라우드 비용 절약 (더 작은 GPU 사용 가능)\")\n",
    "print(\"  - 추론 속도 향상으로 처리량 증가\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea89af6",
   "metadata": {},
   "source": [
    "## 7. 학습 정리 및 다음 단계\n",
    "\n",
    "### 7.1 오늘 배운 내용\n",
    "\n",
    "1. **Quantization 개념**\n",
    "   - 신경망 모델의 가중치를 더 적은 비트로 표현하는 기술\n",
    "   - FP32 → FP16 → INT8 → INT4 순으로 메모리 절약\n",
    "\n",
    "2. **Quantization의 필요성**\n",
    "   - 메모리 부족 문제 해결\n",
    "   - 추론 속도 향상\n",
    "   - 비용 절약\n",
    "\n",
    "3. **실습 내용**\n",
    "   - EXAONE 7.8B 모델을 4bit로 양자화\n",
    "   - 한국어 데이터셋으로 LoRA Fine Tuning\n",
    "   - 메모리 사용량과 성능 비교\n",
    "\n",
    "### 7.2 다음 단계 학습 방향\n",
    "\n",
    "1. **고급 Quantization 기법**\n",
    "   - GPTQ, AWQ 등 다른 양자화 방법\n",
    "   - Mixed-precision 학습\n",
    "\n",
    "2. **더 복잡한 Fine Tuning**\n",
    "   - 대규모 데이터셋 활용\n",
    "   - 다양한 PEFT 기법 (AdaLoRA, QLoRA 등)\n",
    "\n",
    "3. **모델 배포**\n",
    "   - Quantized 모델의 실제 서비스 적용\n",
    "   - ONNX, TensorRT 등 최적화 도구 활용\n",
    "\n",
    "### 7.3 추천 자료\n",
    "\n",
    "- Hugging Face Transformers 문서\n",
    "- PEFT 라이브러리 튜토리얼\n",
    "- BitsAndBytes 공식 문서\n",
    "- 최신 Quantization 연구 논문들\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
